<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CPSC 536C — Algorithms for Convex Optimization</title>
  <style>
    body {
      max-width: 800px;
      margin: 40px auto;
      font-family: sans-serif;
      line-height: 1.6;
      padding: 0 15px;
    }
    h1, h2, h3 {
      color: #333;
    }
    a {
      color: #0055aa;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .section {
      margin-top: 40px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
    }
    th, td {
      text-align: left;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }
  </style>
</head>

<body>

<h1>CPSC 536C: Algorithms for Convex Optimization</h1>
<p><strong>Instructor:</strong> <a href="https://a5ramach.github.io/">Akshay Ramachandran</a></p>
<p><strong>Term:</strong> Winter II, 2025</p>

<div class="section">
  <h2>Course Description</h2>
  <p>
    We will cover the fundamental algorithms used for efficient convex optimization, with a focus on rigorous convergence analysis. 
    At the end, you should have an understanding of why convex optimization algorithms work, and be able to effectively apply these tools to your own research. 
    This course will be different from and complementary to CPSC 536M taught by Michael Friedlander in Term 1. 
  </p>
</div>

<div class="section">
  <h2>Lectures</h2>
  <p>MW 12:30-2pm in DMP 201</p>
</div>

<div class="section">
  <h2>Syllabus</h2>
  <p>Here is a tentative outline of topics covered</p>
  <p>Part I: Convex Analysis</p>
  <ul>
    <li>Convex Sets and Functions</li>
    <li>Examples</li>
    <li>Duality</li>
    <li>Oracle Models: Membership, Separation, Optimization</li>
  </ul>

  <p>Part II: Optimization Algorithms</p>
  <ul>
    <li>Cutting Plane Methods: Center-of-Gravity, Ellipsoid</li>
    <li>First-Order Methods: Gradient Descent, Mirror Descent, Lower Bounds</li>
    <li>Interior Point Methods</li>
  </ul>

  <p>Part III: Applications</p>
  <ul>
    <li>Linear Programming</li>
    <li>Gradient Boosting</li>
    <li>Up to you!</li>
  </ul>

</div>

<div class="section">
  <h2>Announcements</h2>
  <ul>
    <li>Welcome to the course! More information coming soon.</li>
  </ul>
</div>

<div class="section">
  <h2>Schedule</h2>
  <table>
    <tr><th>Date</th><th>Topic</th><th>Notes</th></tr>
    <tr><td>Week 1</td><td>Introduction, Convex sets</td><td>—</td></tr>
    <tr><td>Week 2</td><td>Convex functions, subgradients</td><td>—</td></tr>
  </table>
</div>

<div class="section">
  <h2>Assignments</h2>
  <p>To be posted.</p>
</div>

<div class="section">
  <h2>Resources</h2>
  <ul>
    <li>Boyd & Vandenberghe — <em>Convex Optimization</em></li>
    <li>Nesterov — <em>Introductory Lectures on Convex Optimization</em></li>
    <li>Bubeck - <a href="http://sbubeck.com/BubeckLectureNotes.pdf">Introduction to Online Optimization</a></li>
    <li>Lap Chi Lau - <a href="https://cs.uwaterloo.ca/~lapchi/cs798/">Convexity and Optimization</a></li>
    <li>Yao-Liang Yu - <a href="https://cs.uwaterloo.ca/~y328yu/teaching/794/">Optimization for Data Science</a></li>
  </ul>
</div>

<footer style="margin-top:40px; font-size:0.9em; color:#666;">
  Page design inspired by Michael Friedlander.
</footer>  
</body>
</html>
